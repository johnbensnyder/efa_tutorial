{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EFA Cluster launch\n",
    "\n",
    "This notebook walks through an example of launching an EC2 cluster with EFA to train a deep learning model.\n",
    "\n",
    "Prior to running this notebook, you'll need to do a few things:\n",
    "\n",
    "Setup an EFA enabled seecurity group according to [these instructions](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa-start.html#efa-start-security).\n",
    "\n",
    "Create a placement group in the EC2 console in the availability zone you plan on using.\n",
    "\n",
    "Build an EFA enabled Docker container from the AWS DLC.\n",
    "\n",
    "The `p3dn_efa_east1.yaml` file contains a basic cluster setup.\n",
    "\n",
    "```\n",
    "ImageId: \"ami-0e956fe81fa11d0a9\" # The AWS Deep Learning AMI 42.1 This already has all the EFA drivers\n",
    "InstanceType: \"p3dn.24xlarge\"\n",
    "MinCount: 4 # modify these two lines with how many instances you want\n",
    "MaxCount: 4\n",
    "KeyName: \"jbsnyder-east\" # make sure this key is available in your region\n",
    "IamInstanceProfile:\n",
    "  Name: \"jbsnyder-ec2\" # IAM profile for accessing S3 and other services\n",
    "Placement:\n",
    "  AvailabilityZone: \"us-east-1c\"\n",
    "  GroupName: \"jbsnyder-efa\" # The placement group you created\n",
    "Monitoring:\n",
    "  Enabled: False\n",
    "NetworkInterfaces: # Enables EFA \n",
    "  - SubnetId: \"subnet-c24d759e\"\n",
    "    DeviceIndex: 0\n",
    "    DeleteOnTermination: True\n",
    "    InterfaceType: \"efa\"\n",
    "    Groups:\n",
    "      - \"sg-0b7a6cca873894fff\" # The security group you created\n",
    "BlockDeviceMappings:\n",
    "  - DeviceName: \"/dev/sda1\"\n",
    "    Ebs:\n",
    "      # SnapshotId: \"snap-\"\n",
    "      VolumeSize: 1000 # EBS volume. When reading raw files not tfrecords, like with PT or Dali, might want to use nvme instead\n",
    "      VolumeType: \"gp2\"\n",
    "TagSpecifications:\n",
    "  - ResourceType: \"instance\"\n",
    "    Tags:\n",
    "      - Key: \"Name\"\n",
    "        Value: \"jbsnyder-mrcnn\" # name that appears in the console\n",
    "```\n",
    "\n",
    "Start by reading in the yaml file, creating the ec2 resources, and launching instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import yaml\n",
    "import pprint\n",
    "import subprocess\n",
    "from time import sleep\n",
    "from utils.ssh import SSH, create_ssh_comm, setup_container_communication, create_hostfile\n",
    "\n",
    "config = 'launch_configs/p3dn_efa_east1.yaml'\n",
    "\n",
    "with open(config) as in_config:\n",
    "    config = yaml.safe_load(in_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create resources\n",
    "ec2_session = boto3.Session()\n",
    "ec2_client = ec2_session.client(\"ec2\")\n",
    "ec2_resource = ec2_session.resource(\"ec2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# launch ec2 instances\n",
    "response = ec2_client.run_instances(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i-0a571527be6a8e5d0', 'i-020b19d914d6c57d6', 'i-03c38a7bf17b1c525', 'i-0a5977d718a7d2e12']\n"
     ]
    }
   ],
   "source": [
    "# grab instance ids from what we just launched\n",
    "# example\n",
    "# instances = ['i-0a571527be6a8e5d0', 'i-020b19d914d6c57d6', 'i-03c38a7bf17b1c525', 'i-0a5977d718a7d2e12']\n",
    "# can also manually feed in a list if instances are already running\n",
    "instances = [instance['InstanceId'] for instance in response['Instances']]\n",
    "print(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# wait for instances to become available\n",
    "ready = False\n",
    "while not ready:\n",
    "    sleep(5)\n",
    "    # get current instance status\n",
    "    status = ec2_resource.meta.client.describe_instances(InstanceIds=instances)\n",
    "    # check that instance is running\n",
    "    ready = all([i['State']['Name'] == 'running' for i in status['Reservations'][0]['Instances']])\n",
    "    print(ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ip and dns info\n",
    "public_ips = [instance['PublicIpAddress'] for instance in status['Reservations'][0]['Instances']]\n",
    "public_dns = [instance['PublicDnsName'] for instance in status['Reservations'][0]['Instances']]\n",
    "private_ips = [instance['PrivateIpAddress'] for instance in status['Reservations'][0]['Instances']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ec2-3-84-122-243.compute-1.amazonaws.com',\n",
       " 'ec2-54-198-77-29.compute-1.amazonaws.com',\n",
       " 'ec2-52-91-238-198.compute-1.amazonaws.com',\n",
       " 'ec2-18-206-184-247.compute-1.amazonaws.com']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "public_dns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ssh tool lets you send commands via ssh to all instances simultaneously\n",
    "ssh_client = utils.SSH(public_ips, '/Users/jbsnyder/.aws/jbsnyder-east.pem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first check that instances are up and running\n",
    "# sometime takes a minute longer after instances are ready for them to be fully accessible\n",
    "pci = ssh_client.run_on_all('lspci')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('00:00.0 Host bridge: Intel Corporation 440FX - 82441FX PMC [Natoma]\\n'\n",
      " '00:01.0 ISA bridge: Intel Corporation 82371SB PIIX3 ISA [Natoma/Triton II]\\n'\n",
      " '00:01.3 Non-VGA unclassified device: Intel Corporation 82371AB/EB/MB PIIX4 '\n",
      " 'ACPI (rev 08)\\n'\n",
      " '00:03.0 VGA compatible controller: Amazon.com, Inc. Device 1111\\n'\n",
      " '00:04.0 Non-Volatile memory controller: Amazon.com, Inc. Device 8061\\n'\n",
      " '00:05.0 Ethernet controller: Amazon.com, Inc. Elastic Network Adapter (ENA)\\n'\n",
      " '00:06.0 Ethernet controller: Amazon.com, Inc. Device efa0\\n'\n",
      " '00:16.0 3D controller: NVIDIA Corporation GV100GL [Tesla V100 SXM2 32GB] '\n",
      " '(rev a1)\\n'\n",
      " '00:17.0 3D controller: NVIDIA Corporation GV100GL [Tesla V100 SXM2 32GB] '\n",
      " '(rev a1)\\n'\n",
      " '00:18.0 3D controller: NVIDIA Corporation GV100GL [Tesla V100 SXM2 32GB] '\n",
      " '(rev a1)\\n'\n",
      " '00:19.0 3D controller: NVIDIA Corporation GV100GL [Tesla V100 SXM2 32GB] '\n",
      " '(rev a1)\\n'\n",
      " '00:1a.0 3D controller: NVIDIA Corporation GV100GL [Tesla V100 SXM2 32GB] '\n",
      " '(rev a1)\\n'\n",
      " '00:1b.0 3D controller: NVIDIA Corporation GV100GL [Tesla V100 SXM2 32GB] '\n",
      " '(rev a1)\\n'\n",
      " '00:1c.0 3D controller: NVIDIA Corporation GV100GL [Tesla V100 SXM2 32GB] '\n",
      " '(rev a1)\\n'\n",
      " '00:1d.0 3D controller: NVIDIA Corporation GV100GL [Tesla V100 SXM2 32GB] '\n",
      " '(rev a1)\\n'\n",
      " '00:1e.0 Non-Volatile memory controller: Amazon.com, Inc. NVMe SSD '\n",
      " 'Controller\\n'\n",
      " '00:1f.0 Non-Volatile memory controller: Amazon.com, Inc. NVMe SSD '\n",
      " 'Controller\\n')\n"
     ]
    }
   ],
   "source": [
    "# print out the results of the command from the first node in the cluster\n",
    "# on p3dn this should show a bunch of GPUs\n",
    "pprint.pprint(pci[0]['stdout'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that instances are up and running, we need to set them up so they can communicate with each other. First, create a hostfile on each instance with ip and GPU info for every instance in the cluster. The `create_hostfile` does this automatically.\n",
    "\n",
    "The `create_ssh_comm` creates the rsa keys for all nodes to communicate without logging into each other.\n",
    "\n",
    "Finally, the `setup_container_communication` sets up additional keys to that docker containers on each instance can communicate with docker containers on other instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This \n",
    "create_hostfile(ssh_client, private_ips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_ssh_comm(ssh_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_container_communication(ssh_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command below will log each instance into ECR, and download your docker container. Modify region, repo, and image_name as needed. The aws account will be set automatically based on the account you used to create the cluster. Also note, leave the container name as `mpicont`. This actually has nothing to do with MPI directly, but the container communications tools expect to see a container named mpicont, which is how it directs communication to all containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_pull = '''\n",
    "AWS_ACCOUNT=`aws sts get-caller-identity --query Account --output text` && \\\n",
    "REGION=us-east-1 && \\\n",
    "IMAGE_NAME=dlc-tf24-efa && \\\n",
    "REPO=jbsnyder && \\\n",
    "\n",
    "CONTAINER_IMAGE=${AWS_ACCOUNT}.dkr.ecr.${REGION}.amazonaws.com/${REPO}:${IMAGE_NAME} && \\\n",
    "CONTAINER_NAME=mpicont && \\\n",
    "\n",
    "docker login --username AWS --password $(aws ecr get-login-password --region ${REGION}) \\\n",
    "${AWS_ACCOUNT}.dkr.ecr.${REGION}.amazonaws.com && \\\n",
    "docker pull ${AWS_ACCOUNT}.dkr.ecr.${REGION}.amazonaws.com/${REPO}:${IMAGE_NAME}\n",
    "'''\n",
    "pull_output = ssh_client.run_on_all(docker_pull)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nccl tests are optional, but useful for testing EFA performance. This command clones the nccl test repo to each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "nccl_test = '''\n",
    "cd && \\\n",
    "git clone https://github.com/NVIDIA/nccl-tests.git\n",
    "'''\n",
    "nccl_output = ssh_client.run_on_all(nccl_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few commands are specific to mask rcnn. This downloads the data from S3 to the local EBS volume. At his point it might be useful to mount nvme if you're dealing with small files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data = '''\n",
    "cd && \\\n",
    "mkdir ~/data && \\\n",
    "cd data && \\\n",
    "aws s3 cp --recursive s3://jbsnyder-iad/data/coco/coco_tfrecord/ coco/ && \\\n",
    "aws s3 cp --recursive s3://jbsnyder-iad/data/coco/weights/ weights/\n",
    "'''\n",
    "download_output = ssh_client.run_on_all(download_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This grabs the model repo from github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "password = getpass.getpass()\n",
    "username = 'johnbensnyder'\n",
    "clone_model_repo = '''\n",
    "cd && \\\n",
    "git clone -b sagemaker_cv https://username:{0}@github.com/johnbensnyder/sagemaker_det\n",
    "'''.format(password)\n",
    "repo_output = ssh_client.run_on_all(clone_model_repo)\n",
    "del password\n",
    "del clone_model_repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch the docker container on each node. Modify the lines\n",
    "\n",
    "```\n",
    "-v ~/nccl-tests:/nccl-tests \\\n",
    "-v ~/data:/data \\\n",
    "-v ~/sagemaker_det:/model \\\n",
    "```\n",
    "\n",
    "to whatever directories you want to mount in your container.\n",
    "\n",
    "Make sure to keep `-v /home/ubuntu/ssh_container:/root/.ssh \\` `CONTAINER_NAME=mpicont && \\` and `--device=/dev/infiniband/uverbs0 \\` as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_launch = '''\n",
    "AWS_ACCOUNT=`aws sts get-caller-identity --query Account --output text` && \\\n",
    "REGION=us-east-1 && \\\n",
    "IMAGE_NAME=dlc-tf24-efa && \\\n",
    "REPO=jbsnyder && \\\n",
    "\n",
    "CONTAINER_IMAGE=${AWS_ACCOUNT}.dkr.ecr.${REGION}.amazonaws.com/${REPO}:${IMAGE_NAME} && \\\n",
    "CONTAINER_NAME=mpicont && \\\n",
    "\n",
    "docker run --rm -it -d --gpus all \\\n",
    "                    --name $CONTAINER_NAME \\\n",
    "                    --net=host --uts=host --ipc=host \\\n",
    "                    --ulimit stack=67108864 --ulimit memlock=-1 \\\n",
    "                    --security-opt seccomp=unconfined \\\n",
    "                    -v /home/ubuntu/ssh_container:/root/.ssh \\\n",
    "                    -v ~/nccl-tests:/nccl-tests \\\n",
    "                    -v ~/data:/data \\\n",
    "                    -v ~/sagemaker_det:/model \\\n",
    "                    --device=/dev/infiniband/uverbs0 \\\n",
    "                    $CONTAINER_IMAGE\n",
    "'''\n",
    "launch_output = ssh_client.run_on_all(docker_launch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is the base DLC without my model modifications, need to install some packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip_install = '''\n",
    "docker exec mpicont /bin/bash -c \"pip install yacs tensorflow_addons cython numba tqdm tensorflow_datasets pybind11\"\n",
    "'''\n",
    "pip_output = ssh_client.run_on_all(pip_install)\n",
    "\n",
    "coco_tools = '''\n",
    "docker exec mpicont /bin/bash -c \\\n",
    "\"git clone https://github.com/johnbensnyder/cocoapi && \\\n",
    "\tcd cocoapi/PythonAPI && \\\n",
    "\tpip install -v --no-cache-dir -e .\"\n",
    "'''\n",
    "coco_output = ssh_client.run_on_all(coco_tools)\n",
    "\n",
    "open_cv = '''\n",
    "docker exec mpicont /bin/bash -c \"pip install pip install opencv-python==3.4.11.45\"\n",
    "'''\n",
    "cv_output = ssh_client.run_on_all(open_cv)\n",
    "\n",
    "lib_so = '''\n",
    "docker exec mpicont /bin/bash -c \\\n",
    "\"apt-get update && \\\n",
    "apt-get install ffmpeg libsm6 libxext6  -y\"\n",
    "'''\n",
    "lob_so_output = ssh_client.run_on_all(lib_so)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point it's probably easiest to ssh into one of the nodes in order to launch training. You can use the public DNS from earlier in this notebook.\n",
    "\n",
    "`ssh -i {your key} ubuntu@{public_dns for one of your nodes}`\n",
    "\n",
    "You can then launch training with something like\n",
    "\n",
    "```\n",
    "docker exec mpicont /bin/bash -c \\\n",
    "\"cd /model/tensorflow/tools && \\\n",
    "/opt/amazon/openmpi/bin/mpirun --allow-run-as-root \\\n",
    "-x LD_LIBRARY_PATH=/opt/amazon/efa/lib:/usr/local/lib:/nccl/build/lib:/aws-ofi-nccl/install/lib:$LD_LIBRARY_PATH \\\n",
    "-x NCCL_DEBUG=INFO \\\n",
    "-x RDMAV_FORK_SAFE=1 \\\n",
    "-x FI_PROVIDER=\"efa\" \\\n",
    "--hostfile /root/.ssh/hosts \\\n",
    "--mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 \\\n",
    "--mca btl_vader_single_copy_mechanism none \\\n",
    "--mca oob_tcp_if_include ens5 \\\n",
    "--mca btl_tcp_if_include ens5 \\\n",
    "--oversubscribe \\\n",
    "python train.py --config configs/efa_mrcnn.yaml\"\n",
    "```\n",
    "\n",
    "This is using mpirun, but other distributed strategies like torch.distributed will also work. the `-x LD_LIBRARY_PATH` add all the efa and nccl driver files to the path. `RDMAV_FORK_SAFE=1` is necessary to use nccl with EFA. `FI_PROVIDER=\"efa\"` actually enables EFA. You can also switch off EFA by setting `FI_PROVIDER=\"ena\"`. Note that running commands in this way sometimes means if you stop training early the docker container doesn't get the signal to stop. To prevent this, you can log in to the containeer interactively using `docker exec -it mpicont /bin/bash` then run\n",
    "\n",
    "```\n",
    "cd /model/tensorflow/tools && \\\n",
    "/opt/amazon/openmpi/bin/mpirun --allow-run-as-root \\\n",
    "-x LD_LIBRARY_PATH=/opt/amazon/efa/lib:/usr/local/lib:/nccl/build/lib:/aws-ofi-nccl/install/lib:$LD_LIBRARY_PATH \\\n",
    "-x NCCL_DEBUG=INFO \\\n",
    "-x RDMAV_FORK_SAFE=1 \\\n",
    "-x FI_PROVIDER=\"efa\" \\\n",
    "--hostfile /root/.ssh/hosts \\\n",
    "--mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 \\\n",
    "--mca btl_vader_single_copy_mechanism none \\\n",
    "--mca oob_tcp_if_include ens5 \\\n",
    "--mca btl_tcp_if_include ens5 \\\n",
    "--oversubscribe \\\n",
    "python train.py --config configs/efa_mrcnn.yaml\n",
    "```\n",
    "\n",
    "Or some other command to launch training.\n",
    "\n",
    "Once you're done, you can stop your cluster with\n",
    "\n",
    "`ec2_client.stop_instances(InstanceIds=instances)`\n",
    "\n",
    "0r terminate it entirely with\n",
    "\n",
    "`ec2_client.terminate_instances(InstanceIds=instances)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TerminatingInstances': [{'CurrentState': {'Code': 32,\n",
       "    'Name': 'shutting-down'},\n",
       "   'InstanceId': 'i-0a571527be6a8e5d0',\n",
       "   'PreviousState': {'Code': 16, 'Name': 'running'}},\n",
       "  {'CurrentState': {'Code': 32, 'Name': 'shutting-down'},\n",
       "   'InstanceId': 'i-020b19d914d6c57d6',\n",
       "   'PreviousState': {'Code': 16, 'Name': 'running'}},\n",
       "  {'CurrentState': {'Code': 32, 'Name': 'shutting-down'},\n",
       "   'InstanceId': 'i-03c38a7bf17b1c525',\n",
       "   'PreviousState': {'Code': 16, 'Name': 'running'}},\n",
       "  {'CurrentState': {'Code': 32, 'Name': 'shutting-down'},\n",
       "   'InstanceId': 'i-0a5977d718a7d2e12',\n",
       "   'PreviousState': {'Code': 16, 'Name': 'running'}}],\n",
       " 'ResponseMetadata': {'RequestId': 'e74f120f-24db-4cd7-b81d-ca48e990b82b',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'e74f120f-24db-4cd7-b81d-ca48e990b82b',\n",
       "   'cache-control': 'no-cache, no-store',\n",
       "   'strict-transport-security': 'max-age=31536000; includeSubDomains',\n",
       "   'content-type': 'text/xml;charset=UTF-8',\n",
       "   'transfer-encoding': 'chunked',\n",
       "   'vary': 'accept-encoding',\n",
       "   'date': 'Thu, 08 Apr 2021 21:56:18 GMT',\n",
       "   'server': 'AmazonEC2'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ec2_client.terminate_instances(InstanceIds=instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:studio]",
   "language": "python",
   "name": "conda-env-studio-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
